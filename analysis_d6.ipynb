{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "843d3e8f",
   "metadata": {},
   "source": [
    "# Preparation steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caea904d",
   "metadata": {},
   "source": [
    "## Before Running the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b9bb61",
   "metadata": {},
   "source": [
    "Before running the notebook, install the required packages using the `requirements.txt` file.\n",
    "\n",
    "\n",
    "To do so, run the `pip install -r requirements.txt` command in the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad23b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jiwer\n",
    "#import batchalign\n",
    "import os\n",
    "import pathlib\n",
    "from docx import Document\n",
    "import re\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "from asr_intro_handler import preprocess_transcript_pair\n",
    "from audio_quality_metrics import get_audio_metrics_dataframe\n",
    "\n",
    "#pd.set_option(\"display.max_columns\", 50) \n",
    "#pd.set_option(\"display.max_rows\", 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a73a9f",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37d1b4b",
   "metadata": {},
   "source": [
    "## Setting up the Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e478cf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up the paths\n",
    "\n",
    "path = pathlib.Path().resolve()\n",
    "\n",
    "'''paths to data for analysis'''\n",
    "\n",
    "#path to the directory with the Batchalign transcriptions IN POLISH\n",
    "hypothesis_path_PL = path / \"data\" / \"batchalign\" / \"PL\"\n",
    "\n",
    "#path to the directory with the Batchalign transcriptions IN ENGLISH\n",
    "hypothesis_path_ENG = path / \"data\" / \"batchalign\" / \"ENG\"\n",
    "\n",
    "#path to the directory with the human transcriptions IN POLISH\n",
    "reference_path_PL = path / \"data\" / \"czyszczone_język_polski\" / \"czyszczone_język_polski\" / \"czyszczone_transkrypcje\"\n",
    "\n",
    "#path to the directory with the human transcriptions IN ENGLISH\n",
    "reference_path_ENG = path / \"data\" / \"czyszczone_język_angielski\" / \"czyszczone_język_angielski\" / \"czyszczone_transkrypcje_a\"\n",
    "\n",
    "'''paths to audio files'''\n",
    "\n",
    "#path to the directory where the copy of the cleaned audio files is saved IN POLISH\n",
    "used_audio_path_PL = path / \"data\" /\"czyszczone_język_polski\" / \"czyszczone_język_polski\" / \"czyszczone_nagrania\" \n",
    "\n",
    "#path to the directory where the copy of the cleaned audio files is saved IN ENGLISH\n",
    "used_audio_path_ENG = path / \"data\" / \"czyszczone_język_angielski\" / \"czyszczone_język_angielski\" / \"czyszczone_nagrania_a\"\n",
    "\n",
    "'''paths to backup files'''\n",
    "\n",
    "#path to the directory where the copy of the human transcriptions is saved IN POLISH\n",
    "reference_path_copy_PL = path / \"data_copy\" / \"czyszczone_język_polski\" / \"czyszczone_język_polski\" / \"czyszczone_transkrypcje\"\n",
    "\n",
    "#path to the directory where the copy of the human transcriptions is saved IN ENGLISH\n",
    "reference_path_copy_ENG = path / \"data_copy\" / \"czyszczone_język_angielski\" / \"czyszczone_język_angielski\" / \"czyszczone_transkrypcje_a\"\n",
    "\n",
    "#path to the directory where the copy of the Batchalign transcriptions is saved IN POLISH\n",
    "hypothesis_path_copy_PL = path / \"data_copy\" / \"batchalign\" / \"PL\"\n",
    "\n",
    "#path to the directory where the copy of the Batchalign transcriptions is saved IN ENGLISH\n",
    "hypothesis_path_copy_ENG = path / \"data_copy\" / \"batchalign\" / \"ENG\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb01b1a",
   "metadata": {},
   "source": [
    "#### Script for resetting the files inside the reference paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95680da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #ONLY RUN THIS SCRIPT IF YOU WANT TO RESET THE REFERENCE PATH\n",
    "\n",
    "# # to unncomment the code, select it and press Ctrl + / (or Cmd + / on Mac) \n",
    "# # This script will delete all files in the reference_path and copy files from reference_path_copy_PL and reference_path_copy_ENG to reference_path_PL and reference_path_ENG respectively.\n",
    "# import shutil\n",
    "\n",
    "# # 1. Delete all files from reference_path_PL\n",
    "# for file in reference_path_PL.iterdir():\n",
    "#     if file.is_file():\n",
    "#         file.unlink()\n",
    "\n",
    "# # 2. Copy all files from reference_path_copy_PL to reference_path_PL\n",
    "# for file in reference_path_copy_PL.iterdir():\n",
    "#     if file.is_file():\n",
    "#         target = reference_path_PL / file.name\n",
    "#         shutil.copy2(file, target)\n",
    "        \n",
    "# # 3. Delete all files from reference_path_ENG\n",
    "# for file in reference_path_ENG.iterdir():\n",
    "#     if file.is_file():\n",
    "#         file.unlink()   \n",
    "        \n",
    "# # 4. Copy all files from reference_path_copy_ENG to reference_path_ENG\n",
    "# for file in reference_path_copy_ENG.iterdir():\n",
    "#     if file.is_file():\n",
    "#         target = reference_path_ENG / file.name\n",
    "#         shutil.copy2(file, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3bbf5e",
   "metadata": {},
   "source": [
    "#### Script for resetting the files inside the hypothesis paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d4a8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #ONLY RUN THIS SCRIPT IF YOU WANT TO 'RESET' THE HYPOTHESIS PATH\n",
    "\n",
    "# # to unncomment the code, select it and press Ctrl + / (or Cmd + / on Mac) \n",
    "# # This script will delete all files in the hypothesis_path_PL and hypothesis_path_ENG and copy files from hypothesis_path_copy_PL and hypothesis_path_copy_ENG to hypothesis_path_PL and hypothesis_path_ENG respectively.\n",
    "\n",
    "# import shutil\n",
    "\n",
    "# # 1. Delete all files from hypothesis_path_PL\n",
    "# for file in hypothesis_path_PL.iterdir():\n",
    "#     if file.is_file():\n",
    "#         file.unlink()\n",
    "\n",
    "# # 2. Copy all files from hypothesis_path_copy_PL to hypothesis_path_PL\n",
    "# for file in hypothesis_path_copy_PL.iterdir():\n",
    "#     if file.is_file():\n",
    "#         target = hypothesis_path_PL / file.name\n",
    "#         shutil.copy2(file, target)\n",
    "\n",
    "# # 3. Delete all files from hypothesis_path_ENG\n",
    "# for file in hypothesis_path_ENG.iterdir():\n",
    "#     if file.is_file():\n",
    "#         file.unlink()\n",
    "\n",
    "# # 4. Copy all files from hypothesis_path_copy_ENG to hypothesis_path_ENG\n",
    "# for file in hypothesis_path_copy_ENG.iterdir():\n",
    "#     if file.is_file():\n",
    "#         target = hypothesis_path_ENG / file.name\n",
    "#         shutil.copy2(file, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfb3471",
   "metadata": {},
   "source": [
    "## Preprocessing the reference paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e996a3",
   "metadata": {},
   "source": [
    "### Converting the reference files to .txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eb8ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting all files in the reference_path to a .txt format\n",
    "\n",
    "def convert_to_txt(file_path):\n",
    "    \"\"\"\n",
    "    \n",
    "    !!!!!\n",
    "    Remember to have the backup of the original files before running this function, as it will delete the original files after conversion\n",
    "    !!!!! \n",
    "\n",
    "    Converts .docx, .doc, or .cha files to .txt format and deletes the original file.\n",
    "\n",
    "    \"\"\"\n",
    "    if file_path.suffix.lower() == '.docx':\n",
    "        new_file_path = file_path.with_suffix('.txt')\n",
    "        try:\n",
    "            doc = Document(file_path)\n",
    "            content = \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "            with open(new_file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(content)\n",
    "            os.remove(file_path)\n",
    "            print(f\"Converted and removed: {file_path}\")\n",
    "            return new_file_path\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting {file_path}: {e}\")\n",
    "            return file_path\n",
    "        \n",
    "    elif file_path.suffix.lower() == '.cha':\n",
    "        new_file_path = file_path.with_suffix('.txt')\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f_in:\n",
    "                content = f_in.read()\n",
    "            with open(new_file_path, 'w', encoding='utf-8') as f_out:\n",
    "                f_out.write(content)\n",
    "            os.remove(file_path)\n",
    "            print(f\"Converted and removed: {file_path}\")\n",
    "            return new_file_path\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting {file_path}: {e}\")\n",
    "            return file_path\n",
    "        \n",
    "    elif file_path.suffix.lower() == '.doc':\n",
    "        new_file_path = file_path.with_suffix('.txt')\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f_in:\n",
    "                content = f_in.read()\n",
    "            with open(new_file_path, 'w', encoding='utf-8') as f_out:\n",
    "                f_out.write(content)\n",
    "            os.remove(file_path)\n",
    "            print(f\"Converted and removed: {file_path}\")\n",
    "            return new_file_path\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting {file_path}: {e}\")\n",
    "            return file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c0b923",
   "metadata": {},
   "source": [
    "#### Converting Polish files in the reference_path_PL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3337e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(reference_path_PL):\n",
    "    file_path = reference_path_PL / file\n",
    "    if file_path.is_file() and file_path.suffix.lower() in ['.docx', '.cha']:\n",
    "        convert_to_txt(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eb5649",
   "metadata": {},
   "source": [
    "#### Converting English files in the reference_path_ENG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49316fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(reference_path_ENG):\n",
    "    file_path = reference_path_ENG / file\n",
    "    if file_path.is_file() and file_path.suffix.lower() in ['.docx', '.cha']:\n",
    "        convert_to_txt(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baa569b",
   "metadata": {},
   "source": [
    "### Cleaning the reference files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee322e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_transcript_file(file_path):\n",
    "    \"\"\"\n",
    "    Cleans transcript files by:\n",
    "    - Removing all lines before the first *CHI: or *EXP:\n",
    "    - Removing all lines starting with %com: anywhere in the file\n",
    "    - Removing trailing empty lines and lines like '@End' or similar markers at the end\n",
    "    Overwrites the file in place.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Find the index of the first line starting with *CHI: or *EXP:\n",
    "    start_idx = next(\n",
    "        (i for i, line in enumerate(lines) if line.lstrip().startswith(\"*CHI:\") or line.lstrip().startswith(\"*EXP:\")),\n",
    "        None\n",
    "    )\n",
    "\n",
    "    if start_idx is not None:\n",
    "        # Remove %com: lines after trimming the start\n",
    "        cleaned_lines = [\n",
    "            line for line in lines[start_idx:]\n",
    "            if not line.lstrip().lower().startswith(\"%com:\")\n",
    "        ]\n",
    "        # Remove trailing empty lines and lines like '@End'\n",
    "        while cleaned_lines and (cleaned_lines[-1].strip() == \"\" or cleaned_lines[-1].strip().lower() == \"@end\"):\n",
    "            cleaned_lines.pop()\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            f.writelines(cleaned_lines)\n",
    "        print(f\"Cleaned: {file_path}\")\n",
    "    else:\n",
    "        print(f\"No *CHI: or *EXP: found in {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27370cd6",
   "metadata": {},
   "source": [
    "#### Cleaning the Polish reference files in reference_path_PL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0b6352",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(reference_path_PL):\n",
    "    file_path = reference_path_PL / file\n",
    "    if file_path.is_file() and file_path.suffix.lower() == '.txt':\n",
    "        clean_transcript_file(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1264c6c6",
   "metadata": {},
   "source": [
    "#### Cleaning the English reference files in reference_path_ENG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7877b816",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(reference_path_ENG):\n",
    "    file_path = reference_path_ENG / file\n",
    "    if file_path.is_file() and file_path.suffix.lower() == '.txt':\n",
    "        clean_transcript_file(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2718899f",
   "metadata": {},
   "source": [
    "## Preprocessing the hypothesis paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc43d2d",
   "metadata": {},
   "source": [
    "### Converting the hypothesis files to .txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163502db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_txt(file_path):\n",
    "    \"\"\"\n",
    "    \n",
    "    !!!!!\n",
    "    Remember to have the backup of the original files before running this function, as it will delete the original files after conversion\n",
    "    !!!!! \n",
    "\n",
    "    Converts .docx, .doc, or .cha files to .txt format and deletes the original file.\n",
    "\n",
    "    \"\"\"\n",
    "    if file_path.suffix.lower() == '.docx':\n",
    "        new_file_path = file_path.with_suffix('.txt')\n",
    "        try:\n",
    "            doc = Document(file_path)\n",
    "            content = \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "            with open(new_file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(content)\n",
    "            os.remove(file_path)\n",
    "            print(f\"Converted and removed: {file_path}\")\n",
    "            return new_file_path\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting {file_path}: {e}\")\n",
    "            return file_path\n",
    "        \n",
    "    elif file_path.suffix.lower() == '.cha':\n",
    "        new_file_path = file_path.with_suffix('.txt')\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f_in:\n",
    "                content = f_in.read()\n",
    "            with open(new_file_path, 'w', encoding='utf-8') as f_out:\n",
    "                f_out.write(content)\n",
    "            os.remove(file_path)\n",
    "            print(f\"Converted and removed: {file_path}\")\n",
    "            return new_file_path\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting {file_path}: {e}\")\n",
    "            return file_path\n",
    "        \n",
    "    elif file_path.suffix.lower() == '.doc':\n",
    "        new_file_path = file_path.with_suffix('.txt')\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f_in:\n",
    "                content = f_in.read()\n",
    "            with open(new_file_path, 'w', encoding='utf-8') as f_out:\n",
    "                f_out.write(content)\n",
    "            os.remove(file_path)\n",
    "            print(f\"Converted and removed: {file_path}\")\n",
    "            return new_file_path\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting {file_path}: {e}\")\n",
    "            return file_path\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f68ff5",
   "metadata": {},
   "source": [
    "#### Converting Polish files in the hypothesis_path_PL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0daf19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(hypothesis_path_PL):\n",
    "    file_path = hypothesis_path_PL / file\n",
    "    if file_path.is_file() and file_path.suffix.lower() == '.cha':\n",
    "        convert_to_txt(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb4ba7a",
   "metadata": {},
   "source": [
    "#### Converting English files in the hypothesis_path_ENG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b79ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(hypothesis_path_ENG):\n",
    "    file_path = hypothesis_path_ENG / file\n",
    "    if file_path.is_file() and file_path.suffix.lower() == '.cha':\n",
    "        convert_to_txt(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a240eaf3",
   "metadata": {},
   "source": [
    "### Cleaning the hypothesis files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b21fe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hypothesis_file(file_path):\n",
    "    \"\"\"\n",
    "    Cleans hypothesis files by:\n",
    "    - Removing all lines before the first line starting with *PAR (e.g., *PAR0:, *PAR1:, etc.)\n",
    "    - Removing all timestamp patterns ->  \u0015XXXX_XXXX\u0015\n",
    "    - Removing the last line if it is '@End' (case-insensitive) and any trailing empty lines\n",
    "    Overwrites the file in place.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Find the index of the first line starting with *PAR\n",
    "    start_idx = next(\n",
    "        (i for i, line in enumerate(lines) if line.lstrip().startswith(\"*PAR\")),\n",
    "        None\n",
    "    )\n",
    "\n",
    "    if start_idx is not None:\n",
    "        cleaned_lines = lines[start_idx:]\n",
    "        # Remove timestamp patterns\n",
    "        cleaned_lines = [re.sub(r\"\\x15\\d+_\\d+\\x15\", \"\", line) for line in cleaned_lines]\n",
    "        \n",
    "        \n",
    "        # Remove trailing lines that are empty or '@End'\n",
    "        while cleaned_lines and (cleaned_lines[-1].strip() == \"\" or cleaned_lines[-1].strip().lower() == \"@end\"):\n",
    "            cleaned_lines.pop()\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            f.writelines(cleaned_lines)\n",
    "        print(f\"Cleaned: {file_path}\")\n",
    "    else:\n",
    "        print(f\"No *PAR found in {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f075db1a",
   "metadata": {},
   "source": [
    "#### Cleaning the Polish files in hypothesis_path_PL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0976ee99",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(hypothesis_path_PL):\n",
    "    file_path = hypothesis_path_PL / file\n",
    "    if file_path.is_file() and file_path.suffix.lower() == '.txt':\n",
    "        clean_hypothesis_file(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87059f32",
   "metadata": {},
   "source": [
    "#### Cleaning the English files in hypothesis_path_ENG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673b2eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(hypothesis_path_ENG):\n",
    "    file_path = hypothesis_path_ENG / file\n",
    "    if file_path.is_file() and file_path.suffix.lower() == '.txt':\n",
    "        clean_hypothesis_file(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0282cdab",
   "metadata": {},
   "source": [
    "# Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f3f4d6",
   "metadata": {},
   "source": [
    "### Function for removing speaker labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea138f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_speaker_labels(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove speaker labels from transcripts.\n",
    "    \n",
    "    Removes patterns like:\n",
    "    - *EXP:, *CHI:, *PAR0:, *PAR1:, *PAR2:, etc.\n",
    "    - Any *WORD: pattern at the start of lines or after newlines\n",
    "    \"\"\"\n",
    "    # Remove speaker labels: *WORD: or *WORDdigit:\n",
    "    # Pattern matches: *EXP:, *CHI:, *PAR0:, *PAR1:, etc.\n",
    "    cleaned = re.sub(r'\\*[A-Za-z]+\\d*:\\s*', '', text)\n",
    "    \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f13daea",
   "metadata": {},
   "source": [
    "## Polish Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b08e2d4",
   "metadata": {},
   "source": [
    "### Defining transformations used for Word Error Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21396c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_PL_WER = jiwer.Compose(\n",
    "    [\n",
    "        jiwer.RemoveEmptyStrings(), #removes empty strings\n",
    "        jiwer.ToLowerCase(), #converts all characters to lowercase\n",
    "        jiwer.SubstituteRegexes({r\"\\s+\": \" \"}), # replaces all multipiles, \\n, \\t, etc with a single space \n",
    "        jiwer.Strip(), #removes leading and trailing spaces\n",
    "        jiwer.RemovePunctuation(), #removes punctuation marks (e.g., ., !, ?, etc.)\n",
    "        jiwer.ReduceToListOfListOfWords(), #reduces the transcription to a list of lists of words\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa2818b",
   "metadata": {},
   "source": [
    "### Defining transformations used for Character Error Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641aa05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_PL_CER = jiwer.Compose(\n",
    "    [\n",
    "        jiwer.RemoveEmptyStrings(), #removes empty strings\n",
    "        jiwer.ToLowerCase(), #converts all characters to lowercase\n",
    "        jiwer.SubstituteRegexes({r\"\\s+\": \" \"}), # replaces all multipiles, \\n, \\t, etc with a single space \n",
    "        jiwer.Strip(), #removes leading and trailing spaces\n",
    "        jiwer.RemovePunctuation(), #removes punctuation marks (e.g., ., !, ?, etc.)\n",
    "        jiwer.ReduceToListOfListOfChars(), #reduces the transcription to a list of lists of characters\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403150ef",
   "metadata": {},
   "source": [
    "### Matching the data from reference and hypothesis datasets (Polish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5c4075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all .txt files in both folders\n",
    "ref_files_PL = [f for f in os.listdir(reference_path_PL) if f.endswith('.txt')]\n",
    "hyp_files_PL = [f for f in os.listdir(hypothesis_path_PL) if f.endswith('.txt')]\n",
    "\n",
    "def extract_key(filename):\n",
    "    # Match 2 letters, 3 or 4 digits, underscore, NAp (e.g., Ab123_NAp or Ab1234_NAp)\n",
    "    match = re.match(r\"^[A-Za-z]{2}\\d{3,4}_NAp\", filename)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    return filename  # fallback: whole filename if not matched\n",
    "\n",
    "df_ref_PL = pd.DataFrame({\n",
    "    'key': [extract_key(f) for f in ref_files_PL],\n",
    "    'ref_file': ref_files_PL\n",
    "})\n",
    "df_hyp_PL = pd.DataFrame({\n",
    "    'key': [extract_key(f) for f in hyp_files_PL],\n",
    "    'hyp_file': hyp_files_PL\n",
    "})\n",
    "\n",
    "# Merge to get only matching pairs\n",
    "data_polish = pd.merge(df_ref_PL, df_hyp_PL, on='key')\n",
    "\n",
    "display(data_polish)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa76f3a",
   "metadata": {},
   "source": [
    "### Adding additional information to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b678f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding string columns to the DataFrame\n",
    "# Load raw strings\n",
    "data_polish['ref_string_raw'] = data_polish['ref_file'].apply(lambda x: (reference_path_PL / x).read_text(encoding='utf-8'))\n",
    "data_polish['hyp_string_raw'] = data_polish['hyp_file'].apply(lambda x: (hypothesis_path_PL / x).read_text(encoding='utf-8'))\n",
    "\n",
    "# Remove speaker labels (on raw input)\n",
    "data_polish['ref_string_clean'] = data_polish['ref_string_raw'].apply(remove_speaker_labels)\n",
    "data_polish['hyp_string_clean'] = data_polish['hyp_string_raw'].apply(remove_speaker_labels)\n",
    "\n",
    "# Apply intro-handling\n",
    "processed_refs = []\n",
    "processed_hyps = []\n",
    "has_placeholder_list = []\n",
    "placeholder_text_list = []\n",
    "intro_extracted_list = []\n",
    "intro_length_list = []\n",
    "action_taken_list = []\n",
    "\n",
    "for ref, hyp in zip(data_polish['ref_string_clean'], data_polish['hyp_string_clean']):\n",
    "    pr, ph, meta = preprocess_transcript_pair(ref, hyp, method=mode)\n",
    "    processed_refs.append(pr)\n",
    "    processed_hyps.append(ph)\n",
    "    has_placeholder_list.append(meta['has_placeholder'])\n",
    "    placeholder_text_list.append(\" | \".join(meta['placeholders']))\n",
    "    intro_extracted_list.append(int(meta['intro_extracted']))\n",
    "    intro_length_list.append(len(meta['intro_text']))\n",
    "    action_taken_list.append(meta['action_taken'])\n",
    "\n",
    "data_polish['ref_string'] = processed_refs\n",
    "data_polish['hyp_string'] = processed_hyps\n",
    "data_polish['has_placeholder'] = has_placeholder_list\n",
    "data_polish['placeholder_text'] = placeholder_text_list\n",
    "data_polish['intro_extracted'] = intro_extracted_list\n",
    "data_polish['intro_length_chars'] = intro_length_list\n",
    "data_polish['intro_action'] = action_taken_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68961a9b",
   "metadata": {},
   "source": [
    "### Applying WER- and CER-appropriate transformations to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152c2d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Error Rate (WER) transformations\n",
    "data_polish['ref_transformed_WER'] = data_polish['ref_string'].apply(lambda x: \" \".join(sum(transforms_PL_WER(x), [])))\n",
    "data_polish['hyp_transformed_WER'] = data_polish['hyp_string'].apply(lambda x: \" \".join(sum(transforms_PL_WER(x), [])))\n",
    "\n",
    "# Character Error Rate (CER) transformations\n",
    "data_polish['ref_transformed_CER'] = data_polish['ref_string'].apply(lambda x: \" \".join(sum(transforms_PL_CER(x), [])))\n",
    "data_polish['hyp_transformed_CER'] = data_polish['hyp_string'].apply(lambda x: \" \".join(sum(transforms_PL_CER(x), [])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225d58c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_polish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805311f6",
   "metadata": {},
   "source": [
    "### ***Optional*** Saving the transformed files locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebacf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create directories for transformed files if they don't exist\n",
    "# transformed_ref_path_PL_WER = path / \"transformed_files\" / \"reference\" / \"PL\" / \"WER\"\n",
    "# transformed_hyp_path_PL_WER = path / \"transformed_files\" / \"hypothesis\" / \"PL\" / \"WER\"\n",
    "# transformed_ref_path_PL_WER.mkdir(parents=True, exist_ok=True)\n",
    "# transformed_hyp_path_PL_WER.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# transformed_ref_path_PL_CER = path / \"transformed_files\" / \"reference\" / \"PL\" / \"CER\"\n",
    "# transformed_hyp_path_PL_CER = path / \"transformed_files\" / \"hypothesis\" / \"PL\" / \"CER\"\n",
    "# transformed_ref_path_PL_CER.mkdir(parents=True, exist_ok=True)\n",
    "# transformed_hyp_path_PL_CER.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# # Save transformed reference files with WER transformations\n",
    "# for _, row in data_polish.iterrows():\n",
    "#     ref_output_path = transformed_ref_path_PL_WER / f\"{row['key']}_ref_transformed.txt\"\n",
    "#     with open(ref_output_path, 'w', encoding='utf-8') as f:\n",
    "#         f.write(row['ref_transformed_WER'])\n",
    "    \n",
    "#     # Save transformed hypothesis files\n",
    "#     hyp_output_path = transformed_hyp_path_PL_WER / f\"{row['key']}_hyp_transformed.txt\"\n",
    "#     with open(hyp_output_path, 'w', encoding='utf-8') as f:\n",
    "#         f.write(row['hyp_transformed_WER'])\n",
    "\n",
    "# # Save transformed reference files with CER transformations\n",
    "# for _, row in data_polish.iterrows():\n",
    "#     ref_output_path = transformed_ref_path_PL_CER / f\"{row['key']}_ref_transformed.txt\"\n",
    "#     with open(ref_output_path, 'w', encoding='utf-8') as f:\n",
    "#         f.write(row['ref_transformed_CER'])\n",
    "    \n",
    "#     # Save transformed hypothesis files with CER transformations\n",
    "#     hyp_output_path = transformed_hyp_path_PL_CER / f\"{row['key']}_hyp_transformed.txt\"\n",
    "#     with open(hyp_output_path, 'w', encoding='utf-8') as f:\n",
    "#         f.write(row['hyp_transformed_CER'])\n",
    "\n",
    "\n",
    "# print(f\"Saved {len(data_polish)} transformed reference files to {transformed_ref_path_PL_WER}\")\n",
    "# print(f\"Saved {len(data_polish)} transformed hypothesis files to {transformed_hyp_path_PL_WER}\")\n",
    "# print(f\"Saved {len(data_polish)} transformed reference files to {transformed_ref_path_PL_CER}\")\n",
    "# print(f\"Saved {len(data_polish)} transformed hypothesis files to {transformed_hyp_path_PL_CER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab247f0",
   "metadata": {},
   "source": [
    "### Calculating WER and CER for each reference-hypothesis pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd702ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate WER and CER for each pair after transformation\n",
    "results = []\n",
    "for _, row in data_polish.iterrows():\n",
    "    # with open(reference_path / row['ref_file'], encoding='utf-8') as f:\n",
    "    #     ref_text = f.read()\n",
    "    #     ref_text = remove_speaker_labels(ref_text)\n",
    "    # with open(hypothesis_path / row['hyp_file'], encoding='utf-8') as f:\n",
    "    #     hyp_text = f.read()\n",
    "    #     hyp_text = remove_speaker_labels(hyp_text)\n",
    "\n",
    "    ref_text_PL_WER = row['ref_transformed_WER']\n",
    "    hyp_text_PL_WER = row['hyp_transformed_WER']\n",
    "    ref_text_PL_CER = row['ref_transformed_CER']\n",
    "    hyp_text_PL_CER = row['hyp_transformed_CER']\n",
    "\n",
    "    results.append({\n",
    "        'key': row['key'],\n",
    "        'ref_file': row['ref_file'],\n",
    "        'hyp_file': row['hyp_file'],\n",
    "        'WER': jiwer.wer(ref_text_PL_WER, hyp_text_PL_WER),\n",
    "        'CER': jiwer.cer(ref_text_PL_CER, hyp_text_PL_CER),\n",
    "    })\n",
    "\n",
    "polish_results = pd.DataFrame(results)\n",
    "print(polish_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdc3340",
   "metadata": {},
   "source": [
    "### Calculating mean WER and CER for Polish data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0df7564",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean WER and CER\n",
    "mean_WER = polish_results['WER'].mean()\n",
    "mean_CER = polish_results['CER'].mean()\n",
    "print(f\"Mean WER: {mean_WER:.4f}\")\n",
    "print(f\"Mean CER: {mean_CER:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5925ecf6",
   "metadata": {},
   "source": [
    "## English Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4997d475",
   "metadata": {},
   "source": [
    "### Defining transformations used for Word Error Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a577b63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_ENG_WER = jiwer.Compose(\n",
    "    [\n",
    "        jiwer.ExpandCommonEnglishContractions(), #expands common English contractions like \"don't\" to \"do not\"\n",
    "        jiwer.RemoveEmptyStrings(), #removes empty strings\n",
    "        jiwer.ToLowerCase(), #converts all characters to lowercase\n",
    "        jiwer.SubstituteRegexes({r\"\\s+\": \" \"}), # replaces all multipiles, \\n, \\t, etc with a single space \n",
    "        jiwer.Strip(), #removes leading and trailing spaces\n",
    "        jiwer.RemovePunctuation(), #removes punctuation marks (e.g., ., !, ?, etc.)\n",
    "        jiwer.ReduceToListOfListOfWords(), #reduces the transcription to a list of lists of words\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c922f83",
   "metadata": {},
   "source": [
    "### Defining transformations used for Character Error Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed09185",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformation pipeline\n",
    "\n",
    "transforms_ENG_CER = jiwer.Compose(\n",
    "    [\n",
    "        jiwer.ExpandCommonEnglishContractions(), #expands common English contractions like \"don't\" to \"do not\"\n",
    "        jiwer.RemoveEmptyStrings(), #removes empty strings\n",
    "        jiwer.ToLowerCase(), #converts all characters to lowercase\n",
    "        jiwer.SubstituteRegexes({r\"\\s+\": \" \"}), # replaces all multipiles, \\n, \\t, etc with a single space \n",
    "        jiwer.Strip(), #removes leading and trailing spaces\n",
    "        jiwer.RemovePunctuation(), #removes punctuation marks (e.g., ., !, ?, etc.)\n",
    "        jiwer.ReduceToListOfListOfChars(), #reduces the transcription to a list of lists of characters\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f215c73",
   "metadata": {},
   "source": [
    "### Matching the data from reference and hypothesis datasets (English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac031e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all .txt files in both folders\n",
    "ref_files_ENG = [f for f in os.listdir(reference_path_ENG) if f.endswith('.txt')]\n",
    "hyp_files_ENG = [f for f in os.listdir(hypothesis_path_ENG) if f.endswith('.txt')]\n",
    "\n",
    "def extract_key(filename):\n",
    "    # Match 2 letters, 3 or 4 digits, underscore, NAe (e.g., Ab123_NAe or Ab1234_NAe)\n",
    "    match = re.match(r\"^[A-Za-z]{2}\\d{3,4}_NAe\", filename)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    return filename  # fallback: whole filename if not matched\n",
    "\n",
    "df_ref_ENG = pd.DataFrame({\n",
    "    'key': [extract_key(f) for f in ref_files_ENG],\n",
    "    'ref_file': ref_files_ENG\n",
    "})\n",
    "df_hyp_ENG = pd.DataFrame({\n",
    "    'key': [extract_key(f) for f in hyp_files_ENG],\n",
    "    'hyp_file': hyp_files_ENG\n",
    "})\n",
    "\n",
    "# Merge to get only matching pairs\n",
    "data_english = pd.merge(df_ref_ENG, df_hyp_ENG, on='key')\n",
    "\n",
    "display(data_english)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c461cc",
   "metadata": {},
   "source": [
    "### Adding additional information to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4d1de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding string columns to the DataFrame\n",
    "# Load raw strings\n",
    "data_english['ref_string_raw'] = data_english['ref_file'].apply(lambda x: (reference_path_ENG / x).read_text(encoding='utf-8'))\n",
    "data_english['hyp_string_raw'] = data_english['hyp_file'].apply(lambda x: (hypothesis_path_ENG / x).read_text(encoding='utf-8'))\n",
    "\n",
    "# Remove speaker labels (on raw input)\n",
    "data_english['ref_string_clean'] = data_english['ref_string_raw'].apply(remove_speaker_labels)\n",
    "data_english['hyp_string_clean'] = data_english['hyp_string_raw'].apply(remove_speaker_labels)\n",
    "\n",
    "# Apply intro-handling\n",
    "processed_refs = []\n",
    "processed_hyps = []\n",
    "has_placeholder_list = []\n",
    "placeholder_text_list = []\n",
    "intro_extracted_list = []\n",
    "intro_length_list = []\n",
    "action_taken_list = []\n",
    "\n",
    "for ref, hyp in zip(data_english['ref_string_clean'], data_english['hyp_string_clean']):\n",
    "    pr, ph, meta = preprocess_transcript_pair(ref, hyp, method=mode)\n",
    "    processed_refs.append(pr)\n",
    "    processed_hyps.append(ph)\n",
    "    has_placeholder_list.append(meta['has_placeholder'])\n",
    "    placeholder_text_list.append(\" | \".join(meta['placeholders']))\n",
    "    intro_extracted_list.append(int(meta['intro_extracted']))\n",
    "    intro_length_list.append(len(meta['intro_text']))\n",
    "    action_taken_list.append(meta['action_taken'])\n",
    "\n",
    "data_english['ref_string'] = processed_refs\n",
    "data_english['hyp_string'] = processed_hyps\n",
    "data_english['has_placeholder'] = has_placeholder_list\n",
    "data_english['placeholder_text'] = placeholder_text_list\n",
    "data_english['intro_extracted'] = intro_extracted_list\n",
    "data_english['intro_length_chars'] = intro_length_list\n",
    "data_english['intro_action'] = action_taken_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42baf91b",
   "metadata": {},
   "source": [
    "### Applying WER- and CER-appropriate transformations to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608c455b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Error Rate (WER) transformations\n",
    "data_english['ref_transformed_WER'] = data_english['ref_string'].apply(lambda x: \" \".join(sum(transforms_ENG_WER(x), [])))\n",
    "data_english['hyp_transformed_WER'] = data_english['hyp_string'].apply(lambda x: \" \".join(sum(transforms_ENG_WER(x), [])))\n",
    "\n",
    "# Character Error Rate (CER) transformations\n",
    "data_english['ref_transformed_CER'] = data_english['ref_string'].apply(lambda x: \" \".join(sum(transforms_ENG_CER(x), [])))\n",
    "data_english['hyp_transformed_CER'] = data_english['hyp_string'].apply(lambda x: \" \".join(sum(transforms_ENG_CER(x), [])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa887a6",
   "metadata": {},
   "source": [
    "### ***Optional*** Saving the transformed files locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77602c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create directories for transformed files if they don't exist\n",
    "# transformed_ref_path_ENG_WER = path / \"transformed_files\" / \"reference\" / \"ENG\" / \"WER\"\n",
    "# transformed_hyp_path_ENG_WER = path / \"transformed_files\" / \"hypothesis\" / \"ENG\" / \"WER\"\n",
    "# transformed_ref_path_ENG_WER.mkdir(parents=True, exist_ok=True)\n",
    "# transformed_hyp_path_ENG_WER.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# transformed_ref_path_ENG_CER = path / \"transformed_files\" / \"reference\" / \"ENG\" / \"CER\"\n",
    "# transformed_hyp_path_ENG_CER = path / \"transformed_files\" / \"hypothesis\" / \"ENG\" / \"CER\"\n",
    "# transformed_ref_path_ENG_CER.mkdir(parents=True, exist_ok=True)\n",
    "# transformed_hyp_path_ENG_CER.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# # Save transformed reference files with WER transformations\n",
    "# for _, row in data_english.iterrows():\n",
    "#     ref_output_path = transformed_ref_path_ENG_WER / f\"{row['key']}_ref_transformed.txt\"\n",
    "#     with open(ref_output_path, 'w', encoding='utf-8') as f:\n",
    "#         f.write(row['ref_transformed_WER'])\n",
    "    \n",
    "#     # Save transformed hypothesis files\n",
    "#     hyp_output_path = transformed_hyp_path_ENG_WER / f\"{row['key']}_hyp_transformed.txt\"\n",
    "#     with open(hyp_output_path, 'w', encoding='utf-8') as f:\n",
    "#         f.write(row['hyp_transformed_WER'])\n",
    "\n",
    "# # Save transformed reference files with CER transformations\n",
    "# for _, row in data_english.iterrows():\n",
    "#     ref_output_path = transformed_ref_path_ENG_CER / f\"{row['key']}_ref_transformed.txt\"\n",
    "#     with open(ref_output_path, 'w', encoding='utf-8') as f:\n",
    "#         f.write(row['ref_transformed_CER'])\n",
    "    \n",
    "#     # Save transformed hypothesis files with CER transformations\n",
    "#     hyp_output_path = transformed_hyp_path_ENG_CER / f\"{row['key']}_hyp_transformed.txt\"\n",
    "#     with open(hyp_output_path, 'w', encoding='utf-8') as f:\n",
    "#         f.write(row['hyp_transformed_CER'])\n",
    "\n",
    "\n",
    "# print(f\"Saved {len(data_english)} transformed reference files to {transformed_ref_path_ENG_WER}\")\n",
    "# print(f\"Saved {len(data_english)} transformed hypothesis files to {transformed_hyp_path_ENG_WER}\")\n",
    "# print(f\"Saved {len(data_english)} transformed reference files to {transformed_ref_path_ENG_CER}\")\n",
    "# print(f\"Saved {len(data_english)} transformed hypothesis files to {transformed_hyp_path_ENG_CER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865b936a",
   "metadata": {},
   "source": [
    "### Calculating WER and CER for each reference-hypothesis pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2ee806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate WER and CER for each pair after transformation\n",
    "results = []\n",
    "for _, row in data_english.iterrows():\n",
    "    # with open(reference_path / row['ref_file'], encoding='utf-8') as f:\n",
    "    #     ref_text = f.read()\n",
    "    #     ref_text = remove_speaker_labels(ref_text)\n",
    "    # with open(hypothesis_path / row['hyp_file'], encoding='utf-8') as f:\n",
    "    #     hyp_text = f.read()\n",
    "    #     hyp_text = remove_speaker_labels(hyp_text)\n",
    "\n",
    "    ref_text_ENG_WER = row['ref_transformed_WER']\n",
    "    hyp_text_ENG_WER = row['hyp_transformed_WER']\n",
    "    ref_text_ENG_CER = row['ref_transformed_CER']\n",
    "    hyp_text_ENG_CER = row['hyp_transformed_CER']\n",
    "\n",
    "    results.append({\n",
    "        'key': row['key'],\n",
    "        'ref_file': row['ref_file'],\n",
    "        'hyp_file': row['hyp_file'],\n",
    "        'WER': jiwer.wer(ref_text_ENG_WER, hyp_text_ENG_WER),\n",
    "        'CER': jiwer.cer(ref_text_ENG_CER, hyp_text_ENG_CER),\n",
    "    })\n",
    "\n",
    "english_results = pd.DataFrame(results)\n",
    "print(english_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067aeae6",
   "metadata": {},
   "source": [
    "### Calculating mean WER and CER for English data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8312d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean WER and CER\n",
    "mean_WER = english_results['WER'].mean()\n",
    "mean_CER = english_results['CER'].mean()\n",
    "print(f\"Mean WER: {mean_WER:.4f}\")\n",
    "print(f\"Mean CER: {mean_CER:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ec4123",
   "metadata": {},
   "source": [
    "## Audio data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80416208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOW TO HANDLE INTRO \n",
    "# Choose how to handle introductions\n",
    "# Options: \n",
    "# \"noop\" - ignores intro handling\n",
    "# \"replace\" - replaces placeholder in Human transcript with Batchaling2 generated intro\n",
    "# \"remove\" - deletes intro from Batchaling2\n",
    "mode = \"replace\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cf940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio_quality_metrics import extract_audio_quality_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d8e005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio quality metrics for Polish data\n",
    "\n",
    "audio_metric_df_PL = get_audio_metrics_dataframe(used_audio_path_PL)\n",
    "final_df_PL = pd.merge(polish_results, audio_metric_df_PL, left_on=\"key\", right_on=\"key\")\n",
    "\n",
    "output_name = f\"ASR_results_{mode}_PL.csv\"\n",
    "final_df_PL.to_csv(output_name, index=False)\n",
    "print(\"Saved:\", output_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0efa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio quality metrics for Polish data\n",
    "\n",
    "audio_metric_df_ENG = get_audio_metrics_dataframe(used_audio_path_ENG)\n",
    "final_df_ENG = pd.merge(english_results, audio_metric_df_ENG, left_on=\"key\", right_on=\"key\")\n",
    "\n",
    "output_name = f\"ASR_results_{mode}_ENG.csv\"\n",
    "final_df_ENG.to_csv(output_name, index=False)\n",
    "print(\"Saved:\", output_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "narr-asr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
